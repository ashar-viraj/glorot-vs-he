{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN with Xavier/He (matched init)\n",
        "CIFAR-10, 80/20 split, deep conv stack with activation-appropriate init. Includes weight decay regularization, dropout, early stopping, and CSV logging (detailed + summary) for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_activation(name: str) -> Tuple[nn.Module, str]:\n",
        "    if name == \"relu\":\n",
        "        return nn.ReLU(), \"relu\"\n",
        "    if name == \"tanh\":\n",
        "        return nn.Tanh(), \"tanh\"\n",
        "    raise ValueError(f\"Unsupported activation {name}\")\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, activation: str, dropout_p: float) -> None:\n",
        "        super().__init__()\n",
        "        act, _ = get_activation(activation)\n",
        "        layers: List[nn.Module] = []\n",
        "        layers += [nn.Conv2d(3, 32, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(32, 32, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(32, 32, kernel_size=3, padding=1), act, nn.MaxPool2d(2), nn.Dropout(dropout_p)]\n",
        "        layers += [nn.Conv2d(32, 64, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(64, 64, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(64, 64, kernel_size=3, padding=1), act, nn.MaxPool2d(2), nn.Dropout(dropout_p)]\n",
        "        layers += [nn.Conv2d(64, 128, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(128, 128, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(128, 128, kernel_size=3, padding=1), act, nn.MaxPool2d(2), nn.Dropout(dropout_p)]\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(128 * 4 * 4, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_weights(module: nn.Module, scheme: str, activation_name: str) -> None:\n",
        "    if not isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "        return\n",
        "    if scheme == \"xavier\":\n",
        "        gain = nn.init.calculate_gain(\"tanh\" if activation_name == \"tanh\" else \"relu\")\n",
        "        nn.init.xavier_uniform_(module.weight, gain=gain)\n",
        "    elif scheme == \"he\":\n",
        "        nn.init.kaiming_uniform_(module.weight, nonlinearity=activation_name)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scheme {scheme}\")\n",
        "    if module.bias is not None:\n",
        "        nn.init.zeros_(module.bias)\n",
        "\n",
        "\n",
        "def get_data(batch_size: int, seed: int) -> Tuple[DataLoader, DataLoader]:\n",
        "    transform = transforms.ToTensor()\n",
        "    full_train = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
        "    n_train = int(0.8 * len(full_train))\n",
        "    n_val = len(full_train) - n_train\n",
        "    g = torch.Generator().manual_seed(seed)\n",
        "    train_ds, val_ds = random_split(full_train, [n_train, n_val], generator=g)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def activation_stats(model: nn.Module, x: torch.Tensor) -> Tuple[float, float, float]:\n",
        "    seq = model.features\n",
        "    means, stds, frac_zero = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for layer in seq:\n",
        "            x = layer(x)\n",
        "            if isinstance(layer, (nn.ReLU, nn.Tanh)):\n",
        "                means.append(x.mean().item())\n",
        "                stds.append(x.std().item())\n",
        "                frac_zero.append((x == 0).float().mean().item())\n",
        "    return (float(np.mean(means)) if means else 0.0,\n",
        "            float(np.mean(stds)) if stds else 0.0,\n",
        "            float(np.mean(frac_zero)) if frac_zero else 0.0)\n",
        "\n",
        "\n",
        "def gradient_norm(model: nn.Module) -> float:\n",
        "    norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
        "    return float(np.mean(norms)) if norms else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: Callable, optimizer: torch.optim.Optimizer, device: str) -> float:\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running += loss.item() * x.size(0)\n",
        "    return running / len(loader.dataset)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, criterion: Callable, device: str) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    running = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            running += loss.item() * x.size(0)\n",
        "            preds = out.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return running / len(loader.dataset), correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    batch_size: int = 128\n",
        "    epochs: int = 50\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    dropout: float = 0.1\n",
        "    patience: int = 3\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed: int = 42\n",
        "\n",
        "\n",
        "cfg = Config()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run experiments with activation-matched init; log metrics, activation stats, gradient norm; early stop on stalled val_loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b6bf26",
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(cfg.seed)\n",
        "train_loader, val_loader = get_data(cfg.batch_size, cfg.seed)\n",
        "results = []\n",
        "summaries = []\n",
        "\n",
        "combos = [\n",
        "    (\"relu\", \"he\"),\n",
        "    (\"tanh\", \"xavier\"),\n",
        "]\n",
        "\n",
        "for activation, scheme in combos:\n",
        "    model = ConvNet(activation=activation, dropout_p=cfg.dropout)\n",
        "    model.apply(lambda m: init_weights(m, scheme, activation))\n",
        "    model.to(cfg.device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    last_metrics = None\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, cfg.device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, cfg.device)\n",
        "\n",
        "        sample_x, sample_y = next(iter(val_loader))\n",
        "        sample_x, sample_y = sample_x.to(cfg.device), sample_y.to(cfg.device)\n",
        "        model.zero_grad()\n",
        "        out = model(sample_x)\n",
        "        loss_sample = criterion(out, sample_y)\n",
        "        loss_sample.backward()\n",
        "        grad_norm_val = gradient_norm(model)\n",
        "        act_mean, act_std, frac_zero = activation_stats(model, sample_x)\n",
        "\n",
        "        metrics = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"activation\": activation,\n",
        "            \"init\": scheme,\n",
        "            \"act_mean\": act_mean,\n",
        "            \"act_std\": act_std,\n",
        "            \"frac_zero\": frac_zero,\n",
        "            \"grad_norm\": grad_norm_val,\n",
        "        }\n",
        "        results.append(metrics)\n",
        "        last_metrics = metrics\n",
        "\n",
        "        print(\n",
        "            f\"act={activation} init={scheme} epoch={epoch+1:02d} \"\n",
        "            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc*100:.2f}% \"\n",
        "            f\"act_mean={act_mean:.4f} act_std={act_std:.4f} frac_zero={frac_zero:.4f} grad_norm={grad_norm_val:.4f}\"\n",
        "        )\n",
        "\n",
        "    if last_metrics is not None:\n",
        "        summaries.append(last_metrics)\n",
        "\n",
        "with open(\"good_runs.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))\n",
        "    writer.writeheader()\n",
        "    writer.writerows(results)\n",
        "\n",
        "if summaries:\n",
        "    with open(\"good_runs_summary.csv\", \"w\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(summaries[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(summaries)\n",
        "\n",
        "print(\"Wrote good_runs.csv and good_runs_summary.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}