{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP init comparison (Xavier vs He vs Uniform)\n",
    "Use this notebook to compare weight initializations on MNIST with a small MLP. Follow the notes before each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and seeding helpers\n",
    "Tasks: load dependencies and define a simple `set_seed` for reproducibility. This cell must run first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations and model definition\n",
    "Tasks: choose activation layers and build the MLP architecture (flatten -> 2 hidden layers -> logits). Run after imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name: str) -> Tuple[nn.Module, str]:\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU(), \"relu\"\n",
    "    if name == \"tanh\":\n",
    "        return nn.Tanh(), \"tanh\"\n",
    "    raise ValueError(f\"Unsupported activation {name}\")\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int, activation: str) -> None:\n",
    "        super().__init__()\n",
    "        act_layer, _ = get_activation(activation)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            act_layer,\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            act_layer,\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization schemes and run configuration\n",
    "Tasks: define weight init functions (Xavier/He/uniform) and a dataclass for run hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module: nn.Module, scheme: str, activation_name: str) -> None:\n",
    "    if not isinstance(module, nn.Linear):\n",
    "        return\n",
    "    gain = nn.init.calculate_gain(activation_name)\n",
    "    if scheme == \"xavier\":\n",
    "        nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "    elif scheme == \"he\":\n",
    "        nn.init.kaiming_uniform_(module.weight, nonlinearity=activation_name)\n",
    "    elif scheme == \"uniform\":\n",
    "        fan_in = module.weight.size(1)\n",
    "        bound = 1.0 / fan_in ** 0.5\n",
    "        nn.init.uniform_(module.weight, -bound, bound)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown init scheme {scheme}\")\n",
    "    if module.bias is not None:\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 10\n",
    "    lr: float = 1e-3\n",
    "    hidden_dim: int = 256\n",
    "    init: str = \"xavier\"  # xavier | he | uniform\n",
    "    activation: str = \"relu\"  # relu | tanh\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders\n",
    "Tasks: download MNIST (if needed) and provide training/testing DataLoaders. Run after config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size: int) -> Tuple[DataLoader, DataLoader]:\n",
    "    transform = transforms.ToTensor()\n",
    "    train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "    test_ds = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation helpers\n",
    "Tasks: one epoch of training, and evaluation returning loss and accuracy. Run before starting experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module, loader: DataLoader, criterion: Callable, optimizer: torch.optim.Optimizer, device: str\n",
    ") -> float:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: Callable, device: str) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return running_loss / len(loader.dataset), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run loop and config description\n",
    "Tasks: wire everything together into a training loop and helper to print the chosen config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cfg: RunConfig) -> None:\n",
    "    set_seed(cfg.seed)\n",
    "    train_loader, test_loader = get_data(cfg.batch_size)\n",
    "    _, act_name = get_activation(cfg.activation)\n",
    "    model = MLP(input_dim=28 * 28, hidden_dim=cfg.hidden_dim, num_classes=10, activation=cfg.activation)\n",
    "    model.apply(lambda m: init_weights(m, cfg.init, act_name))\n",
    "    model.to(cfg.device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, cfg.device)\n",
    "        val_loss, val_acc = evaluate(model, test_loader, criterion, cfg.device)\n",
    "        print(\n",
    "            f\"epoch={epoch+1:02d} \"\n",
    "            f\"train_loss={train_loss:.4f} \"\n",
    "            f\"val_loss={val_loss:.4f} \"\n",
    "            f\"val_acc={val_acc*100:.2f}% \"\n",
    "            f\"init={cfg.init}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def describe_run(cfg: RunConfig) -> None:\n",
    "    print(\"Config: \" + str(cfg))\n",
    "    print(\"Run `run(cfg)` in the next cell to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure a run\n",
    "Tasks: set hyperparameters, init scheme, and activation. Edit and execute this cell before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = RunConfig(\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    hidden_dim=256,\n",
    "    init=\"xavier\",  # choose: xavier | he | uniform\n",
    "    activation=\"relu\",  # choose: relu | tanh\n",
    "    seed=42,\n",
    ")\n",
    "describe_run(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Tasks: run the experiment with the selected configuration. Execute after configuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
