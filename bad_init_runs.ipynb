{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN without Xavier/He (naive init)\n",
        "CIFAR-10, 80/20 split, deep conv stack with deliberately bad uniform init (too large / too small bounds). Includes weight decay regularization, dropout, early stopping, and CSV logging (detailed + summary) for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_activation(name: str) -> Tuple[nn.Module, str]:\n",
        "    if name == \"relu\":\n",
        "        return nn.ReLU(), \"relu\"\n",
        "    if name == \"tanh\":\n",
        "        return nn.Tanh(), \"tanh\"\n",
        "    raise ValueError(f\"Unsupported activation {name}\")\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, activation: str, dropout_p: float) -> None:\n",
        "        super().__init__()\n",
        "        act, _ = get_activation(activation)\n",
        "        layers: List[nn.Module] = []\n",
        "        # Block 1: 3x CONV 32 + pool\n",
        "        layers += [nn.Conv2d(3, 32, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(32, 32, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(32, 32, kernel_size=3, padding=1), act, nn.MaxPool2d(2), nn.Dropout(dropout_p)]\n",
        "        # Block 2: 3x CONV 64 + pool\n",
        "        layers += [nn.Conv2d(32, 64, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(64, 64, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(64, 64, kernel_size=3, padding=1), act, nn.MaxPool2d(2), nn.Dropout(dropout_p)]\n",
        "        # Block 3: 3x CONV 128 + pool\n",
        "        layers += [nn.Conv2d(64, 128, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(128, 128, kernel_size=3, padding=1), act]\n",
        "        layers += [nn.Conv2d(128, 128, kernel_size=3, padding=1), act, nn.MaxPool2d(2), nn.Dropout(dropout_p)]\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(128 * 4 * 4, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def naive_uniform_init(module: nn.Module, bound: float) -> None:\n",
        "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "        nn.init.uniform_(module.weight, -bound, bound)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "\n",
        "def get_data(batch_size: int, seed: int) -> Tuple[DataLoader, DataLoader]:\n",
        "    transform = transforms.ToTensor()\n",
        "    full_train = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
        "    n_train = int(0.8 * len(full_train))\n",
        "    n_val = len(full_train) - n_train\n",
        "    g = torch.Generator().manual_seed(seed)\n",
        "    train_ds, val_ds = random_split(full_train, [n_train, n_val], generator=g)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def activation_stats(model: nn.Module, x: torch.Tensor) -> Tuple[float, float, float]:\n",
        "    seq = model.features\n",
        "    means, stds, frac_zero = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for layer in seq:\n",
        "            x = layer(x)\n",
        "            if isinstance(layer, (nn.ReLU, nn.Tanh)):\n",
        "                means.append(x.mean().item())\n",
        "                stds.append(x.std().item())\n",
        "                frac_zero.append((x == 0).float().mean().item())\n",
        "    return (float(np.mean(means)) if means else 0.0,\n",
        "            float(np.mean(stds)) if stds else 0.0,\n",
        "            float(np.mean(frac_zero)) if frac_zero else 0.0)\n",
        "\n",
        "\n",
        "def gradient_norm(model: nn.Module) -> float:\n",
        "    norms = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]\n",
        "    return float(np.mean(norms)) if norms else 0.0"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
